\section{Related Work}
\label{sec:related-work}

The application of artificial intelligence in music production has seen significant growth, shifting from simple information retrieval and symbolic generation (MIDI) toward integrated, intelligent assistance within Digital Audio Workstations (DAWs).
Recent efforts have focused on transforming AI from a generator into a co-creative partner capable of navigating complex software environments.

\subsection*{Current Landscape of Intelligent Music Assistants}
Several prominent systems have emerged that attempt to bridge natural language and DAW control, each with distinct methodologies:

DAWZY: An open-source, voice- and text-driven assistant for the REAPER DAW.
It primarily utilizes code generation to execute low-level tasks, functioning effectively as a ``scripting assistant'' but lacking higher-level production nuance.

FilmComposer: Utilizes a multi-agent system operating a DAW to arrange and mix music specifically for silent films.
While innovative in its use of visual and rhythmic inputs, its scope is limited to autonomous arrangement rather than granular control of existing production elements.

MixAssist: Introduces an audio-language dataset for training models to offer expert mixing advice.
It represents a step toward ``expert systems'' but focuses on providing advice rather than executing real-time control within the session.

Ableton Copilot \& Loop Copilot: These systems represent the current state-of-the-art in session management.
Ableton Copilot is built on the Model Context Protocol (MCP) to manage granular MIDI operations and device control.
Similarly, Loop Copilot integrates LLMs with a Global Attribute Table (GAT) to maintain continuity during iterative editing.

\subsection*{Limitations of Current Approaches (The ``Context'' Gap)}
While the systems above demonstrate the viability of AI in DAWs, they generally rely on the LLM's ability to translate natural language directly into actionable commands via the Model Context Protocol (MCP) or massive context injection.

This approach presents two critical inefficiencies:

Computational Inefficiency: Relying on MCP requires the system to constantly ``read'' and maintain the entire session state to maintain coherence.
This ``brute force'' context management burns tokens rapidly and introduces latency, which is detrimental to the real-time flow of music production.

The ``Theory vs. Production'' Gap: Many existing models are trained on symbolic representations (music theory/MIDI) rather than audio engineering physics.
They struggle to bridge the gap between high-level creative intent (e.g., ``Make it punchy'') and the nuanced signal processing required to achieve it (e.g., specific compression ratios and EQ curves).

\subsection*{Our Contribution: Domain-Specific Knowledge Priors}
Our system addresses these limitations by moving away from generic MCP-based state recording.
Instead, we propose a workflow that incorporates explicit, structured music producer knowledge priors.

Rather than relying on an LLM to ``figure out'' the session state from scratch every time, our system utilizes a domain-aware pipeline:

Intent Parsing: Decoupling the user's creative goal from the technical implementation.

Priors Consultation: referencing a static knowledge base of expert signal chains (e.g., standard vocal processing chains) rather than dynamically generating them via token-heavy reasoning.

Heuristic Execution: Using lightweight Open Sound Control (OSC) messages mapped to these expert priors.

This approach allows the system to provide expert-level guidance that aligns with professional mixing philosophies while significantly reducing the token overhead and latency associated with standard MCP agents.
